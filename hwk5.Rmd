---
output:
  html_document:
    fig_width: 8
---
Elaine Ayo

Math 504 | Homework 5

February 15, 2015

```{r, echo=FALSE}
library(ggplot2)
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  require(grid)

  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

 if (numPlots==1) {
    print(plots[[1]])

  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}

```

2. a. Given the quadratic equation, $f(x) = c + bx + x^TAx$, where A is symmetric. We know for multidimensional system, $\frac{d}{dx}f(x) = \nabla f(x)$ and $\frac{d^2}{d^2x}f(x) = H(f(x))$.

Writing $f(x)$ as a second-order Taylor series about the point $x^{(i)}$ (since all higher-order derivatives would be zero):

$$ f(x) = f(x^{(i)}) + \frac{d}{dx}f(x^{(i)})(x-x^{(i)}) + \frac{\frac{d^2}{d^2x}f(x^{(i)})}{2}(x-x^{(i)})^2 $$

We can find a critical point for $f(x)$ by taking the derivative of the Taylor series with respect to x and setting it equal to zero.

$$ \frac{d}{dx}f(x^{(i)}) + \frac{d^2}{d^2x}f(x^{(i)})(x-x^{(i)}) = 0 $$

$$ x = x^{(i)} - \frac{\frac{d}{dx}f(x^{(i)})}{\frac{d^2}{d^2x}f(x^{(i)})} $$

And the plugging in the first and second derivative equivalents:

$$ x = x^{(i)} - \nabla f(x^{(i)})[Hf(x^{(i)})]^{-1} $$

b.
```{r}
fxn.2b <- list(
  f.x = function(params) return(100*(params[2]-params[1]^2)^2 + (1-params[1])^2),
  grad = function(params) { 
    d.x <- -400*(params[2]-params[1]^2)*params[1]-2*(1-params[1])
    d.y <- 200*(params[2]-params[1]^2)
    return(c(d.x,d.y))
    },
  hess = function(params) {
    d2.x2 <- -400*params[2] + 1200*params[1]^2 + 2
    d2.xy <- -400*params[1]
    d2.y2 <- 200
    return(matrix(c(d2.x2, d2.xy, d2.xy, d2.y2), nrow=2, ncol=2, byrow=TRUE)) 
    },
  norm = function(x) return(sqrt(sum(x)^2)) ,
  graph = function(path) qplot(x=path[,1], y=path[,2], geom="line") + xlab("X") + ylab("Y")
  )
```

```{r}
newton <- function(start, err, fxn=fxn.2b, back = TRUE, multid = TRUE, grapher = TRUE) {
  out <- list("min" = NULL, "steps" = 0, "stepsize" = NULL, "start" = start, "path" = start, "graph" = NULL)
  x.i <- out$start
  
  while(fxn$norm(fxn$grad(x.i)) > err) {
    step <- 1
    M <- fxn$hess(x.i)
    
    #checking to see if any eigenvalues are negative, and if negative, shift by some lambda. 
    #This is only needed for multidimensional case, so it can be turned off for 2-d functions
    if(multid) {
    lambda <- min(eigen(M)$values)
    if(lambda < 0) M <- (lambda)*diag(rep(1,nrow(M))) + M 
    
    #calculate direction with multiple dimensions
    dir <- -solve(M)%*%fxn$grad(x.i)
    } else {
    
    #calculate direction for 2-d function
    dir <-  -fxn$grad(x.i)/M
    }
    
    #backtracking, can also be turned on and off
    if(back) while(fxn$f.x(x.i + step*dir) > fxn$f.x(x.i)) step = step/2
    
    x.i <- x.i + step*dir
    out$path <- rbind(out$path, as.numeric(x.i) )
    out$steps <- out$steps + 1
    out$stepsize <- rbind(out$stepsize, step)
  }
  out$min <- as.numeric(x.i)

  if(grapher) out$graph <- fxn$graph(out$path) #graph path of results
  
  return(out)
}

noback <- newton(c(4,4), err=.005, fxn=fxn.2b, back=FALSE)
back <- newton(c(4,4), err=.005, fxn=fxn.2b, back=TRUE)
```

When I turn on backtracking it takes about 23 steps, and without it takes 3 steps to reach the error level of .005. For steepest ascent, it took 66 steps. Backtracking was slightly closer to the known min of (1,1) with (`r back$min`) versus no-backtracking of (`r noback$min`). Steepest ascent overshot the minimum and landed at (0.9558437, 0.9131749), quite a bit farther off than both versions of Newton's method.

Paths are below:
```{r echo=FALSE, fig.align='center', message=FALSE}
xy2 <- read.table("banana52b.csv", header=FALSE)
plots <- list(
qplot(xy2$V1, xy2$V2, geom="line") + xlab("X") + ylab("Y") + ggtitle("Steepest Ascent"),
back$graph + ggtitle("Newtown's Method \n (With backtracking)"),
noback$graph + ggtitle("Newtown's Method \n (No backtracking)"))
multiplot(plotlist=plots, layout=matrix(c(1,2,3), nrow=1, byrow=TRUE))
```

From the graphs, it is apparent Newton's method without backtracking takes larger leaps than the other two methods, whereas steepest descent bounces back and forth around the minimum until it finally settles there. Newton's method with backtracking has a much smoother path, and once it begins to descend, it lands the closest to the true minimum of all three methods.

c. Simplifying $g(x) = 10x(x-1)(x+1)(x+2)(x-2)$ yields the following functions for $g(x), g'(x), g''(x)$:
```{r}
fxn.2c <- list(
  f.x = function(x) return(10*x^5 - 50*x^3 + 40*x),
  grad = function(x) return(50*x^4 - 150*x^2 + 40), #first derivative
  hess = function(x) return(200*x^3 - 300*x), #second derivative
  norm = function(x) return(sqrt(sum(x)^2)) ,
  graph = function(path) qplot(x=1:length(path[,1]), y=path[,1], geom="line") + xlab("X") + ylab("Y")
  )
```

Then I created a sequence of starting values for x at intervals of .01, and then via an sapply, I ran the newton function written above, which through several parameters can be adjusted to handle $g(x)$. And then the function blew up! It kept breaking when the starting point was 0, because the second derivative of the function, $200x^3 - 300x$ is 0 at x=0, so that makes the Newton's method step function invalid because it requires dividing by 0. So to illustrate the path, I split the vector of start values into either side of 0, and generated the min values that way, with a red line to symbolize $x=0$ on the graph.

```{r}
#sequence of starting values
starts <- seq(-3,3,.0001)

#sapply in two parts to get the path
min1 <- sapply(1:30000, function(x) newton(start=starts[x], err=.0001, fxn=fxn.2c, back=FALSE, multid=FALSE, grapher=FALSE)$min )
min2 <- sapply(30002:length(starts), function(x) newton(start=starts[x], err=.0001, fxn=fxn.2c, back=FALSE, multid=FALSE, grapher=FALSE)$min )

#graph some of this path
f <- Vectorize(function(x) 10*x^5 - 50*x^3 + 40*x)

g.x <- qplot(c(-3,3), fun=f, stat="function", geom="line") + geom_vline(xintercept = 0, color="red", linetype="dotted") + xlab("X") + ylab("g(x)")
path <- qplot(c(starts[1:30000], starts[30002:length(starts)]),c(min1,min2), geom="line") + geom_vline(xintercept = 0, color="red", linetype="dotted") + xlab("Start value for x") + ylab("Critical Points")

multiplot(g.x,path, layout=matrix(c(1,2), nrow=2, byrow=TRUE))
```

The graph of $g(x)$ shows why the path of Newton's method is so erratic -- it has several slight critical points between [-2,2] that causes Netwon's method to find a different critical point depending on start value, even when those start values are really close together. It especially has trouble around -1 and 1, where there are local minimums and maximums very close to one another (hence the erratic behavious in the graph of critical points around here). The behavior is also erratic around 0, where the function has an even more subtle set of local minimum/maximum. Backtracking also got stuck around the problem areas, but I cut it off instead of letting it run a long time, so maybe it would have sorted itself out.

3. Given a $d x d$ square matrix M, and the corresponding identity matrix (the formatting is a little lacking on this one):

$$ \begin{bmatrix} 
\\a_{11} \ a_{12} \ \dots \ a_{1d} 
\\ a_{21} \ a_{22} \ \dots \ a_{2d} 
\\ \vdots \\ a_{d1} \ a_{d2} \ \dots \ a_{dd} 
\end{bmatrix} 
\begin{bmatrix} 
\\ 1 \ 0 \ \dots \ 0 
\\ 0 \ 1 \ \dots \ 0 
\\ \vdots 
\\ 0 \ 0 \ \dots \ 1 
\end{bmatrix}  $$

M can be transformed into the identity matrix by repeating two steps for every column in M:

* Divide each row by the value of M on the diagonal (i.e., the first row by $a_{11}$, the second by $a_{22}$ and so on). 


$$ \begin{bmatrix} 
\\ \frac{a_{11}}{a_{11}} \ \frac{a_{12}}{a_{11}} \ \dots \ \frac{a_{1d}}{a_{11}}
\\  a_{21}  \  a_{22}  \  \dots  \  a_{2d}  
\\ \vdots 
\\  a_{d1}  \  a_{d2}  \ \dots \  a_{dd}  \end{bmatrix} 
\begin{bmatrix} 
\\ \frac{1}{a_{11}} \  0  \ \dots \  0   
\\  0  \  1  \ \dots \  0
\\ \vdots 
\\  0  \   0  \ \dots \  1   
\end{bmatrix} $$

* Subtract the row with the 1 from all other values in its column, after that row is multiplied by the value we're trying to make zero (e.g. R2 - R1 times $a_{21}$, R3 - R1 times $a_{31}$)

$$ \begin{bmatrix} 
\\ 1 \ (\frac{a_{12}}{a_{11}}) \ \dots \ (\frac{a_{1d}}{a_{11}})
\\ (a_{21} - a_{21}) \ (a_{22} - a_{21}\frac{a_{12}}{a_{11}}) \ \dots \ (a_{2d} - a_{21}\frac{a_{1d}}{a_{11}}) 
\\ \vdots 
\\ (a_{d1} - a_{d1}) \ (a_{d2} - a_{d1}\frac{a_{12}}{a_{11}}) \ \dots \ (a_{dd} - a_{d1}\frac{a_{1d}}{a_{11}})
\end{bmatrix} 
\begin{bmatrix} 
\\ (\frac{1}{a_{11}}) \ 0 \ \dots \ 0
\\ (-a_{21}) \ (1 - a_{21}\frac{a_{12}}{a_{11}}) \ \dots \ (-a_{21}\frac{a_{1d}}{a_{11}}) 
\\ \vdots 
\\ (-a_{d1}) \ (-a_{d1}\frac{a_{12}}{a_{11}}) \ \dots \ (1 - a_{d1}\frac{a_{1d}}{a_{11}})
\end{bmatrix}
$$

Once this is completed for each value of M on the diagonal, the resulting matrix on the right will be the identity matrix because by definition, $I = A^{-1}A$. So performing the row operations described above is tantamount to multiplying each side by $A^{-1}$, which yields: $\begin{bmatrix} A^{-1}A = I | A^{-1}I = A^{-1} \end{bmatrix}$.

As for the number of multiplications, the first step for each value in the diagonal requires $2d$ multiplications (since both sides are $d x d$ matrices). The second set of operations requires do another $2d$ multiplications to get the appropriate values for the other $d-1$ rows. In total, this would be $(d-1 + 1)2d = 2d^2$ for one value on the diagonal. 

Since there are  $d$ values in the diagonal, this works out to $2d^3$ multiplications to find the inverse. Finally, multiplying $A^{-1}b$ would be another $d^2$ multiplications since we're multiplying the $d$ rows of $A^{-1}$ by each of the $d$ elements of b. 

Overall, this method would require about $2d^3 + d^2$ calculations, over six times more than Gaussian elimation at $1/3 d^3$. It makes sense that Gaussian elimination is faster because not only does it work on A and b at the same time, it only requires calculating the upper triangular matix, as opposed to performing operations on both sides of the diagonal values.

4. a.
Facts:

* $A$ is symmetric, since $A^T = A$

* Q is orthogonal, and by extension, $Q^T$ is also orthogonal, because they are composed of the eigenvectors and thus $Q^TQ=I$, per results from homework 4.

Based on this, A can be written as:

$$ AQ = A\begin{bmatrix} q^{(1)} \ q^{(2)} \ \dots \ q^{(i)} \end{bmatrix} $$
$$ = \begin{bmatrix} Aq^{(1)} \ Aq^{(2)} \ \dots \ Aq^{(i)} \end{bmatrix} $$

And by definition of the eigenvectors:

$$ \begin{bmatrix} \lambda_1 q^{(1)} \ \lambda_2 q^{(2)} \ \dots \ \lambda_i q^{(i)} \end{bmatrix} = QD $$

... with $Q$ composed on the eigenvectors and $D$ a matrix with the eigenvalues on the diagonal and zeroes elsewhere, as defined in the problem.

$$ AQ = QD \Rightarrow A = QDQ^{-1}$$

Considering that we have shown $Q^TQ = I$, this also implies $Q^T = Q^{-1}$. So finally, $A = QDQ^T$.

b. i.
$$ g(c) = f(\Sigma_{i=1}^3 c_i q^{(i)}) = (\Sigma_{i=1}^3 c_i q^{(i)})^T A (\Sigma_{i=1}^3 c_i q^{(i)}) $$
$$ = \Sigma_{i=1}^3 c_i q^{(i)} \dot \Sigma_{i=1}^3 c_i \lambda_i q^{(i)} $$
$$ = \Sigma_{i=1}^3 \Sigma_{j=1}^3 c_i c_j' \lambda_j q^{(j)} \dot q^{(i)} $$
$$ = \Sigma_{i=1}^3 c^2_i \lambda_i $$
$$ = c^2_1 \lambda_1 + c^2_2 \lambda_2 + c^2_3 \lambda_3 $$



$$ f(Qc) = (Qc)^T A (Qc) $$
$$ = (cQ)^T QDQ^T (Qc) $$
$$ = c^TDc = \begin{bmatrix} c_1 \lambda_1 \\ c_2 \lambda_2 \\ c_3 \lambda_3 \end{bmatrix} c $$
$$ = c^2_1 \lambda_1 + c^2_2 \lambda_2 + c^2_3 \lambda_3 $$

Thus, $g(c) = f(Qc)$.

ii.
$$ f(x) = x^T A x $$

$$ g(Q^T x) = f(\Sigma_{i=1}^3 (Q^T x)_i q^{(i)}) $$

So let's look at what's going on with $Q^T x$:
$$ Q^T x = \begin{bmatrix} 
q^{(11)} \ q^{(12)} \ q^{(13)} 
\\ q^{(21)} \ q^{(22)} \ q^{(23)} 
\\ q^{(31)} \ q^{(32)} \ q^{(33)} 
\end{bmatrix}^T \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix} $$

This means that $Q^T_i$ is equivalent to $q^{(i)T}$:

$$ = f(\Sigma_{i=1}^3 x_i q^{(i)T}q^{(i)}) $$
$$ = f(\Sigma_{i=1}^3 x_i q^{(i)}  \cdot  q^{(i)}) $$
$$ = f(x) = x^T A x $$

Thus, $f(x) = g(Q^Tx)$.

Since $A = QDQ^T$, by subbing in a linear combination of the eigenvectors for $x$ in $f(x)$, $g(c)$ serves to reparameterize $f(x)$ because i. shows it changes $f(x)$ based on a function $\phi$ such that $g(c) = f(\phi(c))$ and ii. shows the inverse of that function (in our case $Q^T = Q^{-1}$), gets us back to $f(x)$.

iii.
$$ g(c) = f(\Sigma_{i=1}^3 c_i q^{(i)}) = (\Sigma_{i=1}^3 c_i q^{(i)})^T A (\Sigma_{i=1}^3 c_i q^{(i)}) $$
$$ = \Sigma_{i=1}^3 c_i q^{(i)} \cdot \Sigma_{i=1}^3 c_i \lambda_i q^{(i)} $$
$$ = \Sigma_{i=1}^3 \Sigma_{j=1}^3 c_i c_j' \lambda_j q^{(j)} \cdot q^{(i)} $$
$$ = \Sigma_{i=1}^3 c^2_i \lambda_i $$

c.
```{r}
A <- matrix(c(2,6,14,6,10,13,14,13,12), nrow=3, ncol=3)

Q <- eigen(A)$vectors

D <- diag(eigen(A)$values)

Q

D

Q %*% D %*% t(Q)
```

Per the theorem we discussed in class, a symmetric matrix $A$ has a maximum if its eigenvalues are positive, a minimum if they are negative, and neither if some are positive and some are negative. For our matrix $A$, D has both positive and negative values, indicating there is no maximum or minimum for $f(x) = c + bx + x^TAx$.





